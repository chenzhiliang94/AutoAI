{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from LLM.helper import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"instruction_induction/\" + \"prompt_picker\" + \"_data/\" + \"all_task_combined\" + \".hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_split = ds[\"train\"].train_test_split(test_size=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 146977\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 314952\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds[\"train\"] = ds_split[\"train\"]\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(task, classification_type, target_loss, epochs=10, to_down_sample=False, down_sample_size=0.2):\n",
    "    ds = load_from_disk(\"instruction_induction/\" + classification_type + \"_data/\" + task + \".hf\")\n",
    "    if to_down_sample:\n",
    "        ds_split = ds[\"train\"].train_test_split(test_size=1-down_sample_size, shuffle=True)\n",
    "        ds[\"train\"] = ds_split[\"train\"]\n",
    "    \n",
    "    model = train_all(ds, classification_type + \"_\" + task, epochs=epochs, target_loss=target_loss)\n",
    "    return model\n",
    "\n",
    "# train_model(\"all_task_combined\", \"example_picker\", 0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_prompt_task_modified(task_name, bounds = torch.stack([torch.ones(2) * 0.01, torch.ones(2) * 0.1]),\n",
    "                    down_sample_size = 0.05, BO_iteration=10, to_use_specific_model = False, sample_size = 1, epochs = 10):\n",
    "    sub_tasks = ['negation', 'num_to_verbal',\n",
    "             'active_to_passive', 'singular_to_plural', 'rhymes',\n",
    "             'second_word_letter', 'sentence_similarity', 'sentiment', 'orthography_starts_with',\n",
    "             'sum', 'synonyms', 'translation_en-de', 'translation_en-es',\n",
    "             'translation_en-fr', 'word_in_context']\n",
    "    openai.api_key = \"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\"\n",
    "    target_loss = [0.5, 0.5]\n",
    "    best_accuracy = -1\n",
    "    all_best_accuracy = []\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(BO_iteration):\n",
    "        start = time.time()\n",
    "\n",
    "        model_to_train_on = \"all_task_combined\"\n",
    "        # if to_use_specific_model:\n",
    "        #     model_to_train_on = task\n",
    "        \n",
    "        overall_accuracy = 0.0\n",
    "        for k in range(sample_size):\n",
    "            model_example_picker = train_model(model_to_train_on, \"example_picker\", target_loss[0], epochs=epochs, to_down_sample=True, down_sample_size=down_sample_size)\n",
    "            model_prompt_picker = train_model(model_to_train_on, \"prompt_picker\", target_loss[1], epochs=epochs, to_down_sample=True, down_sample_size=down_sample_size)\n",
    "\n",
    "            accuracy = {}\n",
    "            for task in sub_tasks:\n",
    "                q_eval,a_eval = get_eval_data(task)\n",
    "        \n",
    "                # deploy model A : classify each example and get a score, get top k example\n",
    "                #q_filtered, a_filtered = modelA_pick_examples(q_eval,a_eval, model = AutoModelForSequenceClassification.from_pretrained(\"./example_picker_\" + task + \"/\", local_files_only=True))\n",
    "                print(\"number of queries before filtering: \", len(q_eval))\n",
    "                q_filtered, a_filtered = modelA_pick_examples(q_eval,a_eval, model = model_example_picker)\n",
    "                print(\"number of queries after filtering: \", len(q_filtered))\n",
    "                if len(q_filtered) == 0:\n",
    "                    q_filtered = q_eval\n",
    "                    a_filtered = a_eval\n",
    "                \n",
    "                # use ape.simple_ape(...) to get a list of prompts (maybe without filtering, so we have bad prompts)\n",
    "                eval_template = \\\n",
    "                \"\"\"Instruction: [PROMPT]\n",
    "                Input: [INPUT]\n",
    "                Output: [OUTPUT]\"\"\"\n",
    "                prompts_generated = ape.ape_to_produce_prompt_autoAI(dataset=(q_filtered, a_filtered), eval_template=eval_template)\n",
    "                print(prompts_generated)\n",
    "\n",
    "                # filter the prompts\n",
    "                prompt_filtered = modelB_pick_prompts(q_filtered, a_filtered, prompts_generated, model = model_prompt_picker)\n",
    "                print(prompt_filtered)\n",
    "\n",
    "                result = ape.evaluate_prompts_autoAI([prompt_filtered], eval_data=(q_eval,a_eval), demo_data=(q_filtered,a_filtered))\n",
    "                print(\"task: \", task)\n",
    "                print(\"accuracy: \", result.sorted()[1][0])\n",
    "                accuracy[task] = result.sorted()[1][0]\n",
    "            print(\"individual task accuracy: \", accuracy)\n",
    "            overall_accuracy = max(overall_accuracy, sum(accuracy.values())/len(accuracy.values()))\n",
    "\n",
    "\n",
    "        if overall_accuracy > best_accuracy:\n",
    "            best_accuracy = overall_accuracy\n",
    "        print(\"X: \", target_loss)\n",
    "        print(\"current accuracy: \", overall_accuracy)\n",
    "        print(\"current best: \", best_accuracy)\n",
    "        all_best_accuracy.append(best_accuracy)\n",
    "        \n",
    "        X.append(target_loss)\n",
    "        y.append([overall_accuracy])\n",
    "        gp = SingleTaskGP(torch.DoubleTensor(X), torch.DoubleTensor(y).double())\n",
    "        mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "        fit_gpytorch_mll(mll)\n",
    "        UCB = UpperConfidenceBound(gp, beta=0.5)\n",
    "        candidate, acq_value = optimize_acqf(\n",
    "        UCB, bounds=bounds, q=1, num_restarts=1, raw_samples=20)\n",
    "        target_loss = candidate[0]\n",
    "        end = time.time()\n",
    "        print(\"time taken for one BO iteration: \", end-start)\n",
    "    print(\"finished. X, y: \")\n",
    "    print(X)\n",
    "    print(y)\n",
    "    return all_best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM.helper import *\n",
    "\n",
    "sub_tasks = ['negation', 'num_to_verbal',\n",
    "             'active_to_passive', 'singular_to_plural', 'rhymes',\n",
    "             'second_word_letter', 'sentence_similarity', 'sentiment', 'orthography_starts_with',\n",
    "             'sum', 'synonyms', 'translation_en-de', 'translation_en-es',\n",
    "             'translation_en-fr', 'word_in_context']\n",
    "\n",
    "total_trials = 1\n",
    "bounds = torch.stack([torch.ones(2) * 0.0001, torch.ones(2) * 3.0])\n",
    "down_sample_size = 0.1\n",
    "BO_iteration = 10\n",
    "to_use_specific_model = False\n",
    "sample_k = 3\n",
    "epochs = 2\n",
    "\n",
    "task = \"all_task_predict_at_once\"\n",
    "loss_space_bo_all_trials = []\n",
    "for trial in range(1):\n",
    "    accuracy = llm_prompt_task_modified(task, bounds = bounds,\n",
    "                        down_sample_size = down_sample_size, BO_iteration=BO_iteration, to_use_specific_model = to_use_specific_model,\n",
    "                        sample_size=sample_k, epochs=epochs)\n",
    "    loss_space_bo_all_trials.append(accuracy)\n",
    "file_name = \"result/llm/\" + str(task) + \".csv\"\n",
    "np.savetxt(file_name, loss_space_bo_all_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating prompts...\n",
      "using accuracy \n",
      "80\n",
      "here\n",
      "80\n",
      "Finished evaluating.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "openai.api_key = \"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\"\n",
    "result = ape.evaluate_prompts_autoAI([\"repeat the sentence\"], eval_data=([\"a\"],[\"b\"]), demo_data=([\"a\"],[\"b\"]),  eval_model='gpt-3.5-turbo',\n",
    "               prompt_gen_model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tasks = ['antonyms', 'diff', 'first_word_letter',\n",
    "             'informal_to_formal', 'larger_animal', 'letters_list', 'taxonomy_animal', 'negation', 'num_to_verbal',\n",
    "             'active_to_passive', 'singular_to_plural', 'rhymes',\n",
    "             'second_word_letter', 'sentence_similarity', 'sentiment', 'orthography_starts_with',\n",
    "             'sum', 'synonyms', 'translation_en-de', 'translation_en-es',\n",
    "             'translation_en-fr', 'word_in_context']\n",
    "import pickle\n",
    "def store_evaluation_data(task_name):\n",
    "    f = open('instruction_induction/raw/induce/' + task_name + '.json')\n",
    "    examples_data = json.load(f)\n",
    "    q = []\n",
    "    a = []\n",
    "\n",
    "    for example_idx in examples_data[\"examples\"]:\n",
    "\n",
    "        if \"input\" not in  examples_data[\"examples\"][str(example_idx)] or \"output\" not in  examples_data[\"examples\"][str(example_idx)]:\n",
    "            print(\"task: \", task_name, \" has weird input output example field, pls check!\")\n",
    "            return q, a\n",
    "    \n",
    "        q.append(examples_data[\"examples\"][str(example_idx)][\"input\"])\n",
    "        a.append(examples_data[\"examples\"][str(example_idx)][\"output\"])\n",
    "    q_eval, a_eval = zip(*random.sample(list(zip(q, a)), int(0.1 * len(q))))\n",
    "    with open(\"instruction_induction/evaluation_data/input_\" + task_name, \"w\") as fp:   #Pickling\n",
    "        json.dump(q_eval, fp)\n",
    "    with open(\"instruction_induction/evaluation_data/output_\" + task_name, \"w\") as fp:   #Pickling\n",
    "        json.dump(a_eval, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_data(task_name):\n",
    "    with open(\"instruction_induction/evaluation_data/input_\" + task_name, \"r\") as fp:\n",
    "        q = json.load(fp)\n",
    "    with open(\"instruction_induction/evaluation_data/output_\" + task_name, \"r\") as fp:\n",
    "        a = json.load(fp)\n",
    "    return q,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,a = get_eval_data(\"antonyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(task, classification_type, target_loss, epochs=10, to_down_sample=False):\n",
    "    ds = load_from_disk(\"instruction_induction/\" + classification_type + \"_data/\" + task + \".hf\")\n",
    "    if to_down_sample:\n",
    "        ds[\"traub\"]\n",
    "    \n",
    "    model = train_all(ds, classification_type + \"_\" + task, epochs=epochs, target_loss=target_loss)\n",
    "    return model\n",
    "\n",
    "# train_model(\"all_task_combined\", \"example_picker\", 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.67, 0.69, 0.69, 0.74, 0.74, 0.74, 0.74]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
