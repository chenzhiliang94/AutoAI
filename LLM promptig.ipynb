{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from Components.ConditionalNormalDistribution import ConditionalNormalDistribution\n",
    "from Components.DifferentiablePolynomial import DifferentiablePolynomial\n",
    "from Models.ModelExponential import ModelExponential\n",
    "from Models.ModelSinCos import ModelSinCos\n",
    "from Models.ModelLogistic import ModelLogistic\n",
    "from Models.ModelLinearRegression import ModelLinearRegression\n",
    "from Models.ModelSigmoid import ModelSigmoid\n",
    "from Composition.SequentialSystem import SequentialSystem\n",
    "from SearchAlgorithm.skeleton import BO_skeleton, BO_graph, BO_graph_local_loss\n",
    "\n",
    "from GraphDecomposition.DirectedFunctionalGraph import DirectedFunctionalGraph\n",
    "from Components.DifferentiablePolynomial import DifferentiablePolynomial\n",
    "from Models.ModelSinCos import ModelSinCos\n",
    "from Models.ModelConstant import ModelConstant\n",
    "from Models.ModelWeightedSum import ModelWeightedSum\n",
    "from Models.ModelWeightedSumThree import ModelWeightedSumThree\n",
    "from Models.ModelExponential import ModelExponential\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from GraphDecomposition.Heuristic import *\n",
    "from helper import *\n",
    "from Plotting.HeatMapLossFunction import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.ModelMNIST import ModelMNIST\n",
    "from mnist.MNISTLoader import *\n",
    "from helper import *\n",
    "\n",
    "ground_truth_param_mnist = {\"Blackbox_DoctorA\": np.array([0.7,0.3]), \"Blackbox_DoctorB\": np.array([0.4,0.4,0.2]), \n",
    "                            \"model_aggregate\": np.array([0.7,0.3])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_healthcare_system(param, noise=0.5, seed=11):  \n",
    "    np.random.seed(seed)\n",
    "    dg_nn = DirectedFunctionalGraph(noise)\n",
    "    \n",
    "    # white box components\n",
    "    dg_nn.add_node(\"model_heart_disease\", component=ModelLogistic(21))\n",
    "    x,y = get_heart_disease_data()\n",
    "    dg_nn.nodes[\"model_heart_disease\"][\"component\"].attach_local_data(x,y)\n",
    " \n",
    "    dg_nn.add_node(\"model_liver_hep\", component=ModelLogistic(14))\n",
    "    x,y = get_hepatitis_data()\n",
    "    \n",
    "    dg_nn.nodes[\"model_liver_hep\"][\"component\"].attach_local_data(x,y)\n",
    "    \n",
    "    dg_nn.add_node(\"model_kidney\", component=ModelLogistic(146))\n",
    "    x,y = get_kidney_data()\n",
    "    dg_nn.nodes[\"model_kidney\"][\"component\"].attach_local_data(x,y)\n",
    "    \n",
    "    dg_nn.add_node(\"model_body_fat\", component=ModelLinearRegression(15))\n",
    "    x,y = get_body_fat_data()\n",
    "    dg_nn.nodes[\"model_body_fat\"][\"component\"].attach_local_data(x,y)\n",
    "\n",
    "    dg_nn.add_node(\"Blackbox_DoctorA\", component=ModelWeightedSum())\n",
    "    dg_nn.add_node(\"Blackbox_DoctorB\", component=ModelWeightedSumThree())\n",
    "    \n",
    "    dg_nn.add_node(\"model_aggregate\", component=ModelWeightedSum())\n",
    "    x,y = get_data_tree(dg_nn.nodes[\"model_aggregate\"][\"component\"], 0, 2, param[\"model_aggregate\"])\n",
    "    dg_nn.nodes[\"model_aggregate\"][\"component\"].attach_local_data(x,y)\n",
    "    \n",
    "    dg_nn.add_edge((\"model_liver_hep\", \"model_kidney\", \"model_body_fat\"), \"Blackbox_DoctorB\")\n",
    "    dg_nn.add_edge((\"model_heart_disease\", \"model_body_fat\"), \"Blackbox_DoctorA\")\n",
    "    dg_nn.add_edge((\"Blackbox_DoctorA\", \"Blackbox_DoctorB\"), \"model_aggregate\")\n",
    "\n",
    "    x,y = get_end_to_end_nn_data(dg_nn, ground_truth_param_mnist, seed=seed)\n",
    "\n",
    "    dg_nn.system_x = x\n",
    "    dg_nn.system_y = y\n",
    "    return dg_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding edge from model_liver_hep to Blackbox_DoctorB\n",
      "adding edge from model_kidney to Blackbox_DoctorB\n",
      "adding edge from model_body_fat to Blackbox_DoctorB\n",
      "adding edge from model_heart_disease to Blackbox_DoctorA\n",
      "adding edge from model_body_fat to Blackbox_DoctorA\n",
      "adding edge from Blackbox_DoctorA to model_aggregate\n",
      "adding edge from Blackbox_DoctorB to model_aggregate\n",
      "setting:  [0.7 0.3]\n",
      "setting:  [0.4 0.4 0.2]\n",
      "setting:  [0.7 0.3]\n"
     ]
    }
   ],
   "source": [
    "data_generation_seed = 3\n",
    "noise_perturb = -1\n",
    "noise_std = 0.5\n",
    "dg_healthcare = create_healthcare_system(ground_truth_param_mnist, noise_perturb, data_generation_seed)\n",
    "dg_healthcare.to_perturb = True\n",
    "dg_healthcare.noise_std = noise_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "nx.draw_networkx(dg_healthcare, font_size=10, node_size=1000, node_color=\"skyblue\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class SpecialTrainer(Trainer):\n",
    "    target_loss = 0.0\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # implement custom logic here\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return ((loss - self.target_loss)**2, outputs) if return_outputs else (loss - self.target_loss)**2\n",
    "\n",
    "f1_score = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1_score.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def train_all(data, model_output_name, epochs=3, target_loss=0.0):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    print(data)\n",
    "    tokenized_data = data.map(preprocess_function, batched=True)\n",
    "    print(tokenized_data[\"test\"])\n",
    "\n",
    "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}   \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=model_output_name,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    )\n",
    "    \n",
    "    trainer = SpecialTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.target_loss = target_loss\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_name)\n",
    "    return model\n",
    "    \n",
    "def modelA_pick_examples(q, a, model):\n",
    "    def combine(q, a):\n",
    "        examples = []\n",
    "        for x,y in zip(q,a):\n",
    "            examples.append(str(x + \". \" + y))\n",
    "        return examples\n",
    "\n",
    "    data = combine(q, a)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    output_q = []\n",
    "    output_a = []\n",
    "\n",
    "    for idx, example in enumerate(data):\n",
    "        inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model.to(\"cpu\")(**inputs).logits\n",
    "            predicted_class_id = logits.argmax().item()\n",
    "        if (predicted_class_id):\n",
    "            output_q.append(q[idx])\n",
    "            output_a.append(a[idx])\n",
    "    return output_q, output_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sub_tasks = ['antonyms', 'diff', 'first_word_letter',\n",
    "             'informal_to_formal', 'larger_animal', 'letters_list', 'taxonomy_animal', 'negation', 'num_to_verbal',\n",
    "             'active_to_passive', 'singular_to_plural', 'rhymes',\n",
    "             'second_word_letter', 'sentence_similarity', 'sentiment', 'orthography_starts_with',\n",
    "             'sum', 'synonyms', 'translation_en-de', 'translation_en-es',\n",
    "             'translation_en-fr', 'word_in_context']\n",
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk, concatenate_datasets\n",
    "def get_data_prompt_picker(task_name):\n",
    "    f = open('instruction_induction/raw/induce/' + task_name + '.json')\n",
    "    correct_promp = open('instruction_induction/annotations/' + task_name + '.json')\n",
    "    examples_data = json.load(f)\n",
    "    prompt_data = json.load(correct_promp)\n",
    "    text = []\n",
    "    label = []\n",
    "\n",
    "    # label 1 (correct prompt)\n",
    "    for example_idx in examples_data[\"examples\"]:\n",
    "\n",
    "        if \"input\" not in  examples_data[\"examples\"][str(example_idx)] or \"output\" not in  examples_data[\"examples\"][str(example_idx)]:\n",
    "            print(\"task: \", task_name, \" has weird input output example field, pls check!\")\n",
    "            return {\"text\":text, \"label\":label}\n",
    "    \n",
    "        q = examples_data[\"examples\"][str(example_idx)][\"input\"]\n",
    "        a = examples_data[\"examples\"][str(example_idx)][\"output\"]\n",
    "        prompts = prompt_data[\"annotations\"]\n",
    "        for prompt in prompts:\n",
    "            instruction = q + \" \" + a + \" \" + prompt\n",
    "            text.append(instruction)\n",
    "            label.append(1)\n",
    "\n",
    "    # label 0 (wrong prompt)\n",
    "    for example_idx in examples_data[\"examples\"]:\n",
    "        \n",
    "        wrong_task = random.choice(sub_tasks)\n",
    "        wrong_f = open('instruction_induction/annotations/' + wrong_task + '.json')\n",
    "        wrong_prompts = json.load(wrong_f)\n",
    "        q = examples_data[\"examples\"][str(example_idx)][\"input\"]\n",
    "        a = examples_data[\"examples\"][str(example_idx)][\"output\"]\n",
    "        wrong_prompts = wrong_prompts[\"annotations\"]\n",
    "        for wrong_prompt in wrong_prompts:\n",
    "            instruction = q + \" \" + a + \" \" + wrong_prompt\n",
    "            text.append(instruction)\n",
    "            label.append(0)\n",
    "\n",
    "    dataset = {\"text\":text, \"label\":label}\n",
    "    return dataset\n",
    "\n",
    "def get_data_example_picker(task_name):\n",
    "    f = open('instruction_induction/raw/induce/' + task_name + '.json')\n",
    "    examples_data = json.load(f)\n",
    "    text = []\n",
    "    label = []\n",
    "\n",
    "    # label 1 (correct example)\n",
    "    for example_idx in examples_data[\"examples\"]:\n",
    "\n",
    "        if \"input\" not in  examples_data[\"examples\"][str(example_idx)] or \"output\" not in  examples_data[\"examples\"][str(example_idx)]:\n",
    "            print(\"task: \", task_name, \" has weird input output example field, pls check!\")\n",
    "            return {\"text\":text, \"label\":label}\n",
    "    \n",
    "        q = examples_data[\"examples\"][str(example_idx)][\"input\"]\n",
    "        a = examples_data[\"examples\"][str(example_idx)][\"output\"]\n",
    "        example = q + \". \" + a\n",
    "        text.append(example)\n",
    "        label.append(1)\n",
    "\n",
    "        wrong_task = random.choice(sub_tasks)\n",
    "        wrong_f = open('instruction_induction/raw/induce/' + wrong_task + '.json')\n",
    "        wrong_example = json.load(wrong_f)\n",
    "        s = len(wrong_example[\"examples\"])\n",
    "        picked_wrong_idx = random.randint(1, s)\n",
    "        q_wrong = wrong_example[\"examples\"][str(picked_wrong_idx)][\"input\"]\n",
    "        a_wrong = wrong_example[\"examples\"][str(picked_wrong_idx)][\"output\"]\n",
    "        wrong_example_one = q + \". \" + a_wrong\n",
    "        text.append(wrong_example_one)\n",
    "        label.append(0)\n",
    "        wrong_example_two = q_wrong + \". \" + a\n",
    "        text.append(wrong_example_two)\n",
    "        label.append(0)\n",
    "\n",
    "    dataset = {\"text\":text, \"label\":label}\n",
    "    return dataset\n",
    "\n",
    "# generate examples and q,a,prompt classification data\n",
    "def generate_all_data():\n",
    "    tasks = sub_tasks\n",
    "    all_data = []\n",
    "    for task in tasks:\n",
    "        print(\"processing task: \", task)\n",
    "        d = get_data_prompt_picker(task)\n",
    "        ds = Dataset.from_dict(d)\n",
    "        all_data.append(ds)\n",
    "        ds = ds.train_test_split(test_size=0.3, shuffle=True)\n",
    "        ds.save_to_disk(\"instruction_induction/prompt_picker_data/\" + task + \".hf\")\n",
    "    ds = concatenate_datasets(all_data)\n",
    "    ds = ds.train_test_split(test_size=0.3, shuffle=True)\n",
    "    ds.save_to_disk(\"instruction_induction/prompt_picker_data/\" + \"all_task_combined\" + \".hf\")\n",
    "    \n",
    "    all_data = []\n",
    "    for task in tasks:\n",
    "        print(\"processing task: \", task)\n",
    "        d = get_data_example_picker(task)\n",
    "        ds = Dataset.from_dict(d)\n",
    "        all_data.append(ds)\n",
    "        ds = ds.train_test_split(test_size=0.3, shuffle=True)\n",
    "        ds.save_to_disk(\"instruction_induction/example_picker_data/\" + task + \".hf\")\n",
    "    ds = concatenate_datasets(all_data)\n",
    "    ds = ds.train_test_split(test_size=0.3, shuffle=True)\n",
    "    ds.save_to_disk(\"instruction_induction/example_picker_data/\" + \"all_task_combined\" + \".hf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from automatic_prompt_engineer import ape\n",
    "openai.api_key = \"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\"\n",
    "# get a list of examples (including bad ones)\n",
    "q = [\"sane\", \"direct\", \"informally\", \"unpopular\", \"subtractive\", \"nonresidential\",\n",
    "         \"inexact\", \"uptown\", \"incomparable\", \"powerful\", \"gaseous\", \"evenly\", \"formality\",\n",
    "         \"deliberately\", \"off\"]\n",
    "\n",
    "a = [\"insane\", \"indirect\", \"formally\", \"popular\", \"additive\", \"residential\",\n",
    "            \"exact\", \"downtown\", \"comparable\", \"powerless\", \"solid\", \"unevenly\", \"informality\",\n",
    "            \"accidentally\", \"on\"]\n",
    "task = \"antonyms\"\n",
    "\n",
    "# deploy model A : classify each example and get a score, get top k example\n",
    "print(\"./example_picker_\" + task + \"/\")\n",
    "q_filtered, a_filtered = modelA_pick_examples(q,a, model = AutoModelForSequenceClassification.from_pretrained(\"./example_picker_\" + task + \"/\", local_files_only=True))\n",
    "\n",
    "# use ape.simple_ape(...) to get a list of prompts (maybe without filtering, so we have bad prompts)\n",
    "eval_template = \\\n",
    "\"\"\"Instruction: [PROMPT]\n",
    "Input: [INPUT]\n",
    "Output: [OUTPUT]\"\"\"\n",
    "prompts_generated = ape.ape_to_produce_prompt(dataset=(q_filtered, a_filtered), eval_template=eval_template)\n",
    "\n",
    "# deploy model B : classify each prompt and example to get a score, get top prompt.\n",
    "prompt_filtered = modelB_pick_prompts(q_filtered, a_filtered, prompts_generated, model = AutoModelForSequenceClassification.from_pretrained(\"./prompt_picker_\" +task+ \"/\", local_files_only=True))\n",
    "\n",
    "# pass prompt to LLM, with evaluation examples to get answer and score.\n",
    "#           res = evaluate.evalute_prompts(prompts, eval_template, eval_data, demos_template, few_shot_data,\n",
    "#           conf['evaluation']['method'], conf['evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(task, classification_type):\n",
    "    ds = load_from_disk(\"instruction_induction/\" + classification_type + \"_data/\" + task + \".hf\")\n",
    "    model = train_all(ds, \"example_picker_\" + task)\n",
    "    return model\n",
    "\n",
    "def modelB_pick_prompts(q_filtered, a_filtered, prompts_generated, model):\n",
    "    def combine(q, a):\n",
    "        examples = []\n",
    "        for x,y in zip(q,a):\n",
    "            examples.append(str(x + \". \" + y))\n",
    "        return examples\n",
    "\n",
    "    examples = combine(q_filtered, a_filtered)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    prompt_scores = []\n",
    "    for idx, prompt in enumerate(prompts_generated):\n",
    "        prompt_score_on_examples = 0\n",
    "        for idx, example in enumerate(examples):\n",
    "            #print(\"input: \", example + \". \" + prompt)\n",
    "            inputs = tokenizer(example + \". \" + prompt, return_tensors=\"pt\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model.to(\"cpu\")(**inputs).logits\n",
    "                predicted_class_id = logits.argmax().item()\n",
    "                #print(\"prediction: \", predicted_class_id)\n",
    "            if (predicted_class_id):\n",
    "                prompt_score_on_examples +=1 # may need to just change to logit score? idk\n",
    "        prompt_scores.append(prompt_score_on_examples)\n",
    "    #print(prompt_scores)\n",
    "    highest_prompt_score = max(prompt_scores)\n",
    "    idx_of_best_prompts = [i for i, j in enumerate(prompt_scores) if j == highest_prompt_score]\n",
    "    best_idx = random.choice(idx_of_best_prompts)\n",
    "    best_prompt = prompts_generated[best_idx]\n",
    "    return best_prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 30222\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 12953\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 12953\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='708' max='708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [708/708 08:38, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>0.983041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.011658</td>\n",
       "      <td>0.983041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.011642</td>\n",
       "      <td>0.983041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5443\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2333\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e1cd9899b846fe97c4c4f32610e9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142072ece5344ddfb886398d637edacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2333\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 01:35, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.095724</td>\n",
       "      <td>0.827423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.056600</td>\n",
       "      <td>0.051594</td>\n",
       "      <td>0.890123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.047135</td>\n",
       "      <td>0.895062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 57751\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 24751\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b304e377576d496494976e3a28324a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/57751 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbeac5ef4ce4baab104086eea5e110d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24751 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 24751\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1353' max='1353' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1353/1353 16:18, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.014949</td>\n",
       "      <td>0.979457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>0.979457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.012725</td>\n",
       "      <td>0.979457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10395\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4455\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddde905ad6624affb6a6098f8e669d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f2d47ca16a4850b2f9fd78991c9337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4455\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 03:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.026563</td>\n",
       "      <td>0.929515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.018924</td>\n",
       "      <td>0.948619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>0.955060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 38622\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16553\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221ed981dda6468d80df81e09e730d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38622 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c6da17ae63484f956819dacb91b010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 16553\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='903' max='903' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [903/903 10:59, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.015445</td>\n",
       "      <td>0.978842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.015188</td>\n",
       "      <td>0.978842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.978842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6942\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2976\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb91f5a6b004a928ccd1b2be1f8f5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47f5c297b8a452d84bb1982aef80a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2976\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/162 01:58, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.108119</td>\n",
       "      <td>0.801636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.101824</td>\n",
       "      <td>0.807090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.100889</td>\n",
       "      <td>0.809110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 149\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 64\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69ffe8b2c744fabaaca1118406084a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2044a2ef73439d8a1fb202f748f3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 64\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.462298</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.453040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.447466</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 31\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687a574f7e5e49648e17ba1ce6ecc710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c6875529724c84af76710085736052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 14\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.479993</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.480035</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.480785</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 35329\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 15142\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89537f6e01f2404d9b6fa0b1e06b06ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754fea14dbdc40629ccbe665898bc9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 15142\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='828' max='828' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [828/828 10:06, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.018382</td>\n",
       "      <td>0.975320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.015642</td>\n",
       "      <td>0.975320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.013396</td>\n",
       "      <td>0.975136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6358\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2726\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1bf03b1b3a48b7ae3eb3bcf83791fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8846e54938c84a33acb40643d4a5b843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2726 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2726\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 01:48, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.991940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.995685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.993007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 38530\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 16513\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519067e9f17143fb90d923fd11d6e0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f05a2b2c08a4f3bb9f600331d11b3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 16513\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='903' max='903' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [903/903 11:07, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.017875</td>\n",
       "      <td>0.975246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>0.975246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.011736</td>\n",
       "      <td>0.974593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6942\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2976\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657a61fd96764f769b0641f00ef838d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93bc95f33324d1da3bbc7034176cb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2976\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/162 01:57, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.129900</td>\n",
       "      <td>0.112509</td>\n",
       "      <td>0.798357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.107220</td>\n",
       "      <td>0.797370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.108361</td>\n",
       "      <td>0.797683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 35112\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 15049\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca2943dbb0a442f891fe0484bd0bbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a249338fa2f41489c90fa019a71a00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15049 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 15049\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='822' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [822/822 10:06, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.016945</td>\n",
       "      <td>0.977315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.975737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2700\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e597c1bfa747a39e36f54f875a2e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fc198505bd4ef493d801a78f1cd68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2700\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 01:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.999451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9917\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4251\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1eb532c4a34a74bde3b48c6bb060ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814c8305366f433191274952dafce0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4251\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [231/231 02:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.014418</td>\n",
       "      <td>0.979053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>0.979053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1898\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 814\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a0b38027da4e97b75fbac222578480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143664300e514806ae9b55edf1b438b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 814\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.152376</td>\n",
       "      <td>0.980843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.026590</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 108731\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 46599\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7656dd7cecac43f5b5bee6c19afc6dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4fd943aa7f4f668fef4c70f42b2125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 46599\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2547' max='2547' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2547/2547 30:57, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>0.977173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.014663</td>\n",
       "      <td>0.977173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 20790\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8910\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4407dbffb2594433aeeaf410a35b69ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20790 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca313e627db34c238e8203b1443e68ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 8910\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='486' max='486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [486/486 05:56, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>0.957443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.957746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9870\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4231\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e960817584a406e8d02f01b34f3b09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1d7f637ecc457cbdfc31d013e9dda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4231 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4231\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [231/231 02:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.020490</td>\n",
       "      <td>0.974009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.019769</td>\n",
       "      <td>0.974009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.019182</td>\n",
       "      <td>0.974009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1890\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 810\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42634952d536494a8554fbcaf148f4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1890 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ada843cb2174e14918bf293efe1b191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 810\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.353300</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.994434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.996283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.007691</td>\n",
       "      <td>0.998138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 21322\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9138\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d27793d4ea4befaaf502d75f54f9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc0c293c423401cbbc34bb4362196c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 9138\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='498' max='498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [498/498 06:07, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.980017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.013116</td>\n",
       "      <td>0.980017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.013005</td>\n",
       "      <td>0.980017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4080\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1749\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d2f2016a7e459ca4d8e5008b7bdc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29aed094eb7a47559c28ae08e8cddeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1749\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.147800</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.980525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.992340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.991511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 32882\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 14093\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6baa60c04147b587b36e84e89f0451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b65b4559af34a11a037b5862ddf95cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 14093\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='771' max='771' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [771/771 09:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>0.976662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.012835</td>\n",
       "      <td>0.976662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.975019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2700\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f561e1e01bf49e7ba1f71b65bc5c3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c38e205946747f8bde58ef15ef24f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2700\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 01:51, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.154949</td>\n",
       "      <td>0.746587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.129509</td>\n",
       "      <td>0.781236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.118112</td>\n",
       "      <td>0.795937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 34019\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 14580\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ecae653b724aafb80c27abb155bdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34019 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51399e77807e44708716bb36d40fb2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 14580\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='798' max='798' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [798/798 09:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.976948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.013143</td>\n",
       "      <td>0.976948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.976948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6942\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2976\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13af6b985eb14172a1bc45e2585353b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d49a91f8004befbd2b99aac21f5564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2976\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [162/162 02:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.118255</td>\n",
       "      <td>0.783307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.113848</td>\n",
       "      <td>0.785107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.107100</td>\n",
       "      <td>0.114419</td>\n",
       "      <td>0.784472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 35585\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 15251\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9be2d5927914730ad97d12eceb7ce42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f280a5c3a2d4a7cb47a07cb23462494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 15251\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='834' max='834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [834/834 09:16, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.011697</td>\n",
       "      <td>0.977088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>0.977088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.977088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7803\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3345\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adc5d3a770548a2a693726bf6ecf48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7803 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f530f5dd8d6b4ff993378467b8180e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3345\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/183 02:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.018236</td>\n",
       "      <td>0.958685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.015566</td>\n",
       "      <td>0.959964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.015838</td>\n",
       "      <td>0.959964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 12883\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5522\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749bf9a1a8ec48d08cba0125427f8352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12883 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47f56d8f274eddbfbffd40a0e771c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5522\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 03:19, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.015320</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.978500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2450\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1051\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9525d5b2400a4e89bd1fdcfb3f3db923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0e0b9358304059aca33e907b9c5644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1051 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1051\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 00:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.086077</td>\n",
       "      <td>0.912863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.042635</td>\n",
       "      <td>0.916553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.030890</td>\n",
       "      <td>0.937063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 32977\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 14133\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f1157168fd4c78a2b71164ebf7854b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32977 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367ccda2e2f74f45a93b088e5b521eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 14133\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='771' max='771' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [771/771 09:15, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.978798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.013763</td>\n",
       "      <td>0.978798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.013586</td>\n",
       "      <td>0.978798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6300\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2700\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9171b5631ee54a1591c0938e000d8a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa27187c21a4df2a3a8e728d4de4b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2700\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [147/147 01:47, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.991568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.994369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.994376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 54257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 23254\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c22a6c9ff544614bb1dda6464cd7dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b59fa506ce74c3fbd2e692a59d7beba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 23254\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1272' max='1272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1272/1272 14:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.013909</td>\n",
       "      <td>0.978391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.013831</td>\n",
       "      <td>0.978391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>0.978391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10395\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4455\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db359615573949689f20c03af025c684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e112b200874fab88e6a47b76102af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4455 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 4455\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 03:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.031932</td>\n",
       "      <td>0.927289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.017135</td>\n",
       "      <td>0.954289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.015171</td>\n",
       "      <td>0.958189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 24327\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 10427\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72927a68650d4da29b721380a9a19b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1a20aa18d449f6a99518e79f8e085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 10427\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 06:36, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.014995</td>\n",
       "      <td>0.979469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.014686</td>\n",
       "      <td>0.979469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>0.979469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4666\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8373c831f68c4c5dbb13e193c8af0c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf829df98f04ffabc8fa6b2a67b33b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 01:21, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.201065</td>\n",
       "      <td>0.719074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.104723</td>\n",
       "      <td>0.799712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 26903\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 11530\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24319b6257054fdbb2a382c3fcc3e868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c227d4c43ecf4d42b771b9aa0a6c011a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 11530\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 06:55, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.011706</td>\n",
       "      <td>0.979272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.011189</td>\n",
       "      <td>0.979272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.979272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5896\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2528\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cb7364d2d04aee82785150662472a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5896 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e48a13dfd4f4b448f6d64d5c7791bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2528\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 01:18, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>0.785294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.121117</td>\n",
       "      <td>0.797456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>0.800197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 22137\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9488\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cd8f56cb544dfe9f01c502af631d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c84a8a3ada042e3a6ef69852be4256a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9488 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 9488\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='519' max='519' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [519/519 06:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.979512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007211</td>\n",
       "      <td>0.979512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>0.979512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5693\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2440\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2907d16e5764201acc8cbef1738007b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5693 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148092eb962c42c5a9663195a4922776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2440\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 01:21, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.172100</td>\n",
       "      <td>0.145576</td>\n",
       "      <td>0.756513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>0.117389</td>\n",
       "      <td>0.788344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 22920\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 9824\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caeb6d1526094cf99ec21bc81be8832b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866931e2be084f1dbdcc7b793f7d72f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9824 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 9824\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='537' max='537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [537/537 06:26, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.008460</td>\n",
       "      <td>0.977263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.977263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>0.977263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 5903\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 2530\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8e9a40fab244e68b62c46542ec420f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5903 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814946a923b74cc1bd896ce5eda7d5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2530\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='138' max='138' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [138/138 01:30, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.126847</td>\n",
       "      <td>0.783853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.086306</td>\n",
       "      <td>0.800231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.082332</td>\n",
       "      <td>0.812114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50432\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 21615\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb983c5f69448f88ba93858952e852b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50432 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52d71dbd2594428b2c128e4fbb685f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21615 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 21615\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1182' max='1182' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1182/1182 13:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>0.978860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.016170</td>\n",
       "      <td>0.978860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.013364</td>\n",
       "      <td>0.978860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8576\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3676\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ba9d79672e4adb8f19bb72e79591e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5112121360024b0fb1446d42065a8358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3676\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [201/201 02:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.957104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.957104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.957104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/chenzhil/AutoAI/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "def train_model(task, classification_type):\n",
    "    ds = load_from_disk(\"instruction_induction/\" + classification_type + \"_data/\" + task + \".hf\")\n",
    "    model = train_all(ds, classification_type + \"_\" + task, epochs=3)\n",
    "    return model\n",
    "\n",
    "for task in sub_tasks:\n",
    "    m = train_model(task, \"prompt_picker\")\n",
    "    m = train_model(task, \"example_picker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  subtractive. additive.  produce an output that is the opposite of the input.\n",
      "prediction:  1\n",
      "input:  nonresidential. residential.  produce an output that is the opposite of the input.\n",
      "prediction:  1\n",
      "input:  inexact. exact.  produce an output that is the opposite of the input.\n",
      "prediction:  1\n",
      "input:  subtractive. additive.  reverse the word.\n",
      "prediction:  0\n",
      "input:  nonresidential. residential.  reverse the word.\n",
      "prediction:  0\n",
      "input:  inexact. exact.  reverse the word.\n",
      "prediction:  0\n",
      "input:  subtractive. additive.  reverse the input-output pairs.\n",
      "prediction:  0\n",
      "input:  nonresidential. residential.  reverse the input-output pairs.\n",
      "prediction:  0\n",
      "input:  inexact. exact.  reverse the input-output pairs.\n",
      "prediction:  0\n",
      "input:  subtractive. additive.  \"reverse the input.\"\n",
      "prediction:  0\n",
      "input:  nonresidential. residential.  \"reverse the input.\"\n",
      "prediction:  0\n",
      "input:  inexact. exact.  \"reverse the input.\"\n",
      "prediction:  0\n",
      "input:  subtractive. additive.  reverse the input.\n",
      "prediction:  0\n",
      "input:  nonresidential. residential.  reverse the input.\n",
      "prediction:  0\n",
      "input:  inexact. exact.  reverse the input.\n",
      "prediction:  0\n",
      "input:  subtractive. additive.  take the input word and reverse it.\n",
      "prediction:  0\n",
      "input:  nonresidential. residential.  take the input word and reverse it.\n",
      "prediction:  0\n",
      "input:  inexact. exact.  take the input word and reverse it.\n",
      "prediction:  0\n",
      "input:  subtractive. additive.  reverse the word order of the input.\n",
      "prediction:  0\n",
      "input:  nonresidential. residential.  reverse the word order of the input.\n",
      "prediction:  0\n",
      "input:  inexact. exact.  reverse the word order of the input.\n",
      "prediction:  0\n",
      "input:  subtractive. additive.  reverse the order of the letters in each word.\n",
      "prediction:  0\n",
      "input:  nonresidential. residential.  reverse the order of the letters in each word.\n",
      "prediction:  0\n",
      "input:  inexact. exact.  reverse the order of the letters in each word.\n",
      "prediction:  0\n",
      "input:  subtractive. additive.  \"find the antonym for each word.\"\n",
      "prediction:  1\n",
      "input:  nonresidential. residential.  \"find the antonym for each word.\"\n",
      "prediction:  1\n",
      "input:  inexact. exact.  \"find the antonym for each word.\"\n",
      "prediction:  1\n",
      "[3, 0, 0, 0, 0, 0, 0, 0, 3]\n",
      "best prompt picked:   produce an output that is the opposite of the input.\n"
     ]
    }
   ],
   "source": [
    "prompts_generated = [' produce an output that is the opposite of the input.', ' reverse the word.', ' reverse the input-output pairs.', ' \"reverse the input.\"', ' reverse the input.', ' take the input word and reverse it.', ' reverse the word order of the input.', ' reverse the order of the letters in each word.', ' \"find the antonym for each word.\"']\n",
    "q_filtered = [\"subtractive\", \"nonresidential\",\n",
    "         \"inexact\"]\n",
    "\n",
    "a_filtered = [\"additive\", \"residential\",\n",
    "            \"exact\"]\n",
    "prompt_filtered = modelB_pick_prompts(q_filtered, a_filtered, prompts_generated, model = AutoModelForSequenceClassification.from_pretrained(\"./prompt_picker_antonyms/\"))\n",
    "print(\"best prompt picked: \", prompt_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
